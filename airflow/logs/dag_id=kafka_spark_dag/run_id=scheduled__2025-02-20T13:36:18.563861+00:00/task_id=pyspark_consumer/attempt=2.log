[2025-02-21T13:39:00.525+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: kafka_spark_dag.pyspark_consumer scheduled__2025-02-20T13:36:18.563861+00:00 [queued]>
[2025-02-21T13:39:00.722+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: kafka_spark_dag.pyspark_consumer scheduled__2025-02-20T13:36:18.563861+00:00 [queued]>
[2025-02-21T13:39:00.723+0000] {taskinstance.py:1361} INFO - Starting attempt 2 of 2
[2025-02-21T13:39:00.884+0000] {taskinstance.py:1382} INFO - Executing <Task(DockerOperator): pyspark_consumer> on 2025-02-20 13:36:18.563861+00:00
[2025-02-21T13:39:00.936+0000] {standard_task_runner.py:57} INFO - Started process 1502 to run task
[2025-02-21T13:39:00.985+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'kafka_spark_dag', 'pyspark_consumer', 'scheduled__2025-02-20T13:36:18.563861+00:00', '--job-id', '35', '--raw', '--subdir', 'DAGS_FOLDER/dag_kafka_spark.py', '--cfg-path', '/tmp/tmpaz7s2kni']
[2025-02-21T13:39:00.996+0000] {standard_task_runner.py:85} INFO - Job 35: Subtask pyspark_consumer
[2025-02-21T13:39:01.382+0000] {task_command.py:416} INFO - Running <TaskInstance: kafka_spark_dag.pyspark_consumer scheduled__2025-02-20T13:36:18.563861+00:00 [running]> on host d5862e138d8d
[2025-02-21T13:39:02.117+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='kafka_spark_dag' AIRFLOW_CTX_TASK_ID='pyspark_consumer' AIRFLOW_CTX_EXECUTION_DATE='2025-02-20T13:36:18.563861+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-20T13:36:18.563861+00:00'
[2025-02-21T13:39:02.438+0000] {docker.py:343} INFO - Starting docker container from image rappel-conso/spark:latest
[2025-02-21T13:39:04.061+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m13:39:04.04 [0m[38;5;2mINFO [0m ==>
[2025-02-21T13:39:04.079+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m13:39:04.07 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2025-02-21T13:39:04.111+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m13:39:04.08 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2025-02-21T13:39:04.131+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m13:39:04.12 [0m[38;5;2mINFO [0m ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.
[2025-02-21T13:39:04.141+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m13:39:04.13 [0m[38;5;2mINFO [0m ==>
[2025-02-21T13:39:04.280+0000] {docker.py:413} INFO - 
[2025-02-21T13:39:18.378+0000] {docker.py:413} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-02-21T13:39:18.598+0000] {docker.py:413} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
[2025-02-21T13:39:18.599+0000] {docker.py:413} INFO - The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2025-02-21T13:39:18.618+0000] {docker.py:413} INFO - org.postgresql#postgresql added as a dependency
[2025-02-21T13:39:18.620+0000] {docker.py:413} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2025-02-21T13:39:18.624+0000] {docker.py:413} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-55f78161-f493-4f9c-a9b2-b3ad66c5c654;1.0
[2025-02-21T13:39:18.633+0000] {docker.py:413} INFO - confs: [default]
[2025-02-21T13:39:21.712+0000] {docker.py:413} INFO - found org.postgresql#postgresql;42.5.4 in central
[2025-02-21T13:39:22.233+0000] {docker.py:413} INFO - found org.checkerframework#checker-qual;3.5.0 in central
[2025-02-21T13:39:25.514+0000] {docker.py:413} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central
[2025-02-21T13:39:27.665+0000] {docker.py:413} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
[2025-02-21T13:39:28.094+0000] {docker.py:413} INFO - found org.apache.kafka#kafka-clients;3.4.1 in central
[2025-02-21T13:39:28.309+0000] {docker.py:413} INFO - found org.lz4#lz4-java;1.8.0 in central
[2025-02-21T13:39:28.519+0000] {docker.py:413} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2025-02-21T13:39:29.679+0000] {docker.py:413} INFO - found org.slf4j#slf4j-api;2.0.7 in central
[2025-02-21T13:39:31.989+0000] {docker.py:413} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2025-02-21T13:39:32.335+0000] {docker.py:413} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2025-02-21T13:39:34.208+0000] {docker.py:413} INFO - found commons-logging#commons-logging;1.1.3 in central
[2025-02-21T13:39:34.421+0000] {docker.py:413} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-02-21T13:39:37.355+0000] {docker.py:413} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2025-02-21T13:39:37.490+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.5.4/postgresql-42.5.4.jar ...
[2025-02-21T13:39:37.865+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.postgresql#postgresql;42.5.4!postgresql.jar (442ms)
[2025-02-21T13:39:37.929+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar ...
[2025-02-21T13:39:38.210+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0!spark-sql-kafka-0-10_2.12.jar (334ms)
[2025-02-21T13:39:38.295+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar ...
[2025-02-21T13:39:38.417+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.checkerframework#checker-qual;3.5.0!checker-qual.jar (190ms)
[2025-02-21T13:39:38.484+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar ...
[2025-02-21T13:39:38.584+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0!spark-token-provider-kafka-0-10_2.12.jar (162ms)
[2025-02-21T13:39:38.657+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[2025-02-21T13:39:40.096+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (1499ms)
[2025-02-21T13:39:40.175+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2025-02-21T13:39:40.244+0000] {docker.py:413} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (133ms)
[2025-02-21T13:39:40.315+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2025-02-21T13:39:40.423+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (175ms)
[2025-02-21T13:39:40.483+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[2025-02-21T13:39:55.350+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (14918ms)
[2025-02-21T13:39:55.409+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2025-02-21T13:39:55.685+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (319ms)
[2025-02-21T13:39:55.753+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[2025-02-21T13:39:56.670+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (990ms)
[2025-02-21T13:39:56.731+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
[2025-02-21T13:39:56.864+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (184ms)
[2025-02-21T13:39:56.957+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[2025-02-21T13:40:02.436+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (5567ms)
[2025-02-21T13:40:02.499+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2025-02-21T13:40:02.571+0000] {docker.py:413} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (118ms)
:: resolution report :: resolve 18793ms :: artifacts dl 25153ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
[2025-02-21T13:40:02.576+0000] {docker.py:413} INFO - org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
	org.checkerframework#checker-qual;3.5.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.postgresql#postgresql;42.5.4 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   13  |   13  |   13  |   0   ||   13  |   13  |
	---------------------------------------------------------------------
[2025-02-21T13:40:02.599+0000] {docker.py:413} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-55f78161-f493-4f9c-a9b2-b3ad66c5c654
	confs: [default]
[2025-02-21T13:40:02.790+0000] {docker.py:413} INFO - 13 artifacts copied, 0 already retrieved (58001kB/191ms)
[2025-02-21T13:40:03.802+0000] {docker.py:413} INFO - 25/02/21 13:40:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-02-21T13:40:05.956+0000] {docker.py:413} INFO - python3: can't open file '/opt/bitnami/spark/scripts/spark_pgsql.py': [Errno 2] No such file or directory
[2025-02-21T13:40:06.029+0000] {docker.py:413} INFO - 25/02/21 13:40:06 INFO ShutdownHookManager: Shutdown hook called
[2025-02-21T13:40:06.037+0000] {docker.py:413} INFO - 25/02/21 13:40:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-16d2b1e4-b29f-47ee-b69f-4e9d0e7826e7
[2025-02-21T13:40:06.967+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 486, in execute
    return self._run_image()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 360, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 421, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 2}
[2025-02-21T13:40:06.978+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=kafka_spark_dag, task_id=pyspark_consumer, execution_date=20250220T133618, start_date=20250221T133900, end_date=20250221T134006
[2025-02-21T13:40:07.018+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 35 for task pyspark_consumer (Docker container failed: {'StatusCode': 2}; 1502)
[2025-02-21T13:40:07.073+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-02-21T13:40:07.193+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
