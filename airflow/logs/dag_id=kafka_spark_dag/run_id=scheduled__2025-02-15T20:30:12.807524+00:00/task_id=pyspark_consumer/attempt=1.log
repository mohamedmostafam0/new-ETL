[2025-02-16T20:30:51.953+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: kafka_spark_dag.pyspark_consumer scheduled__2025-02-15T20:30:12.807524+00:00 [queued]>
[2025-02-16T20:30:51.969+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: kafka_spark_dag.pyspark_consumer scheduled__2025-02-15T20:30:12.807524+00:00 [queued]>
[2025-02-16T20:30:51.970+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2025-02-16T20:30:51.989+0000] {taskinstance.py:1382} INFO - Executing <Task(DockerOperator): pyspark_consumer> on 2025-02-15 20:30:12.807524+00:00
[2025-02-16T20:30:52.005+0000] {standard_task_runner.py:57} INFO - Started process 1130 to run task
[2025-02-16T20:30:52.014+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'kafka_spark_dag', 'pyspark_consumer', 'scheduled__2025-02-15T20:30:12.807524+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/dag_kafka_spark.py', '--cfg-path', '/tmp/tmp1ktrzdjh']
[2025-02-16T20:30:52.015+0000] {standard_task_runner.py:85} INFO - Job 17: Subtask pyspark_consumer
[2025-02-16T20:30:52.138+0000] {task_command.py:416} INFO - Running <TaskInstance: kafka_spark_dag.pyspark_consumer scheduled__2025-02-15T20:30:12.807524+00:00 [running]> on host 8d98db3ff19d
[2025-02-16T20:30:52.364+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='kafka_spark_dag' AIRFLOW_CTX_TASK_ID='pyspark_consumer' AIRFLOW_CTX_EXECUTION_DATE='2025-02-15T20:30:12.807524+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-15T20:30:12.807524+00:00'
[2025-02-16T20:30:52.438+0000] {docker.py:343} INFO - Starting docker container from image rappel-conso/spark:latest
[2025-02-16T20:30:52.451+0000] {docker.py:351} WARNING - Using remote engine or docker-in-docker and mounting temporary volume from host is not supported. Falling back to `mount_tmp_dir=False` mode. You can set `mount_tmp_dir` parameter to False to disable mounting and remove the warning
[2025-02-16T20:30:53.581+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m20:30:53.57 [0m[38;5;2mINFO [0m ==>
[2025-02-16T20:30:53.586+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m20:30:53.58 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2025-02-16T20:30:53.588+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m20:30:53.58 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2025-02-16T20:30:53.593+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m20:30:53.59 [0m[38;5;2mINFO [0m ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.
[2025-02-16T20:30:53.596+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m20:30:53.59 [0m[38;5;2mINFO [0m ==>
[2025-02-16T20:30:53.639+0000] {docker.py:413} INFO - 
[2025-02-16T20:31:05.449+0000] {docker.py:413} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-02-16T20:31:06.149+0000] {docker.py:413} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2025-02-16T20:31:06.177+0000] {docker.py:413} INFO - org.postgresql#postgresql added as a dependency
[2025-02-16T20:31:06.181+0000] {docker.py:413} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2025-02-16T20:31:06.183+0000] {docker.py:413} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-261c578a-1fd1-47c6-8e55-b5d22916277f;1.0
	confs: [default]
[2025-02-16T20:31:09.141+0000] {docker.py:413} INFO - found org.postgresql#postgresql;42.5.4 in central
[2025-02-16T20:31:09.363+0000] {docker.py:413} INFO - found org.checkerframework#checker-qual;3.5.0 in central
[2025-02-16T20:31:13.309+0000] {docker.py:413} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central
[2025-02-16T20:31:15.817+0000] {docker.py:413} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
[2025-02-16T20:31:16.091+0000] {docker.py:413} INFO - found org.apache.kafka#kafka-clients;3.4.1 in central
[2025-02-16T20:31:16.482+0000] {docker.py:413} INFO - found org.lz4#lz4-java;1.8.0 in central
[2025-02-16T20:31:16.795+0000] {docker.py:413} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2025-02-16T20:31:18.276+0000] {docker.py:413} INFO - found org.slf4j#slf4j-api;2.0.7 in central
[2025-02-16T20:31:20.781+0000] {docker.py:413} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2025-02-16T20:31:21.668+0000] {docker.py:413} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2025-02-16T20:31:24.032+0000] {docker.py:413} INFO - found commons-logging#commons-logging;1.1.3 in central
[2025-02-16T20:31:24.291+0000] {docker.py:413} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-02-16T20:31:27.331+0000] {docker.py:413} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2025-02-16T20:31:27.675+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.5.4/postgresql-42.5.4.jar ...
[2025-02-16T20:31:28.242+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.postgresql#postgresql;42.5.4!postgresql.jar (644ms)
[2025-02-16T20:31:28.361+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar ...
[2025-02-16T20:31:28.574+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0!spark-sql-kafka-0-10_2.12.jar (309ms)
[2025-02-16T20:31:28.670+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar ...
[2025-02-16T20:31:28.853+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.checkerframework#checker-qual;3.5.0!checker-qual.jar (260ms)
[2025-02-16T20:31:28.967+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar ...
[2025-02-16T20:31:29.062+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0!spark-token-provider-kafka-0-10_2.12.jar (188ms)
[2025-02-16T20:31:29.146+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[2025-02-16T20:31:30.558+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (1470ms)
[2025-02-16T20:31:30.612+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2025-02-16T20:31:30.700+0000] {docker.py:413} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (134ms)
[2025-02-16T20:31:30.767+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2025-02-16T20:31:30.865+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (153ms)
[2025-02-16T20:31:30.920+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[2025-02-16T20:31:45.338+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (14473ms)
[2025-02-16T20:31:45.390+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2025-02-16T20:31:45.798+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (457ms)
[2025-02-16T20:31:45.854+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[2025-02-16T20:31:47.055+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (1253ms)
[2025-02-16T20:31:47.103+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
[2025-02-16T20:31:47.201+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (137ms)
[2025-02-16T20:31:47.249+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[2025-02-16T20:32:00.996+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (13752ms)
[2025-02-16T20:32:01.068+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2025-02-16T20:32:01.399+0000] {docker.py:413} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (412ms)
[2025-02-16T20:32:01.408+0000] {docker.py:413} INFO - :: resolution report :: resolve 21377ms :: artifacts dl 33847ms
	:: modules in use:
[2025-02-16T20:32:01.411+0000] {docker.py:413} INFO - com.google.code.findbugs#jsr305;3.0.0 from central in [default]
[2025-02-16T20:32:01.441+0000] {docker.py:413} INFO - commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
	org.checkerframework#checker-qual;3.5.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.postgresql#postgresql;42.5.4 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
[2025-02-16T20:32:01.477+0000] {docker.py:413} INFO - org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
[2025-02-16T20:32:01.518+0000] {docker.py:413} INFO - |      default     |   13  |   13  |   13  |   0   ||   13  |   13  |
	---------------------------------------------------------------------
[2025-02-16T20:32:01.673+0000] {docker.py:413} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-261c578a-1fd1-47c6-8e55-b5d22916277f
[2025-02-16T20:32:01.685+0000] {docker.py:413} INFO - confs: [default]
[2025-02-16T20:32:02.316+0000] {docker.py:413} INFO - 13 artifacts copied, 0 already retrieved (58001kB/629ms)
[2025-02-16T20:32:05.811+0000] {docker.py:413} INFO - 25/02/16 20:32:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-02-16T20:32:09.930+0000] {docker.py:413} INFO - python3: can't open file '/opt/bitnami/spark/scripts/spark_pgsql.py': [Errno 2] No such file or directory
[2025-02-16T20:32:10.079+0000] {docker.py:413} INFO - 25/02/16 20:32:10 INFO ShutdownHookManager: Shutdown hook called
[2025-02-16T20:32:10.104+0000] {docker.py:413} INFO - 25/02/16 20:32:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-eb8d6114-9b7e-4b39-85b7-40d45246fa76
[2025-02-16T20:32:11.336+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py", line 268, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.8/site-packages/requests/models.py", line 1021, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http://docker-proxy:2375/v1.47/containers/create

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 348, in _run_image
    return self._run_image_with_mounts([*self.mounts, tmp_mount], add_tmp_variable=True)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 375, in _run_image_with_mounts
    self.container = self.cli.create_container(
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/container.py", line 431, in create_container
    return self.create_container_from_config(config, name, platform)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/container.py", line 448, in create_container_from_config
    return self._result(res, True)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py", line 274, in _result
    self._raise_for_status(response)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py", line 270, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.APIError: 400 Client Error for http://docker-proxy:2375/v1.47/containers/create: Bad Request ("invalid mount config for type "bind": bind source path does not exist: /tmp/airflowtmp7783uunv")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 486, in execute
    return self._run_image()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 357, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 421, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 2}
[2025-02-16T20:32:11.375+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=kafka_spark_dag, task_id=pyspark_consumer, execution_date=20250215T203012, start_date=20250216T203051, end_date=20250216T203211
[2025-02-16T20:32:11.443+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 17 for task pyspark_consumer (Docker container failed: {'StatusCode': 2}; 1130)
[2025-02-16T20:32:11.498+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-02-16T20:32:11.569+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
