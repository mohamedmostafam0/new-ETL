[2025-02-16T22:13:27.433+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: kafka_spark_dag.pyspark_consumer manual__2025-02-16T22:11:30.822967+00:00 [queued]>
[2025-02-16T22:13:27.504+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: kafka_spark_dag.pyspark_consumer manual__2025-02-16T22:11:30.822967+00:00 [queued]>
[2025-02-16T22:13:27.507+0000] {taskinstance.py:1361} INFO - Starting attempt 2 of 2
[2025-02-16T22:13:27.606+0000] {taskinstance.py:1382} INFO - Executing <Task(DockerOperator): pyspark_consumer> on 2025-02-16 22:11:30.822967+00:00
[2025-02-16T22:13:27.662+0000] {standard_task_runner.py:57} INFO - Started process 1761 to run task
[2025-02-16T22:13:27.700+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'kafka_spark_dag', 'pyspark_consumer', 'manual__2025-02-16T22:11:30.822967+00:00', '--job-id', '34', '--raw', '--subdir', 'DAGS_FOLDER/dag_kafka_spark.py', '--cfg-path', '/tmp/tmp2ee5tqio']
[2025-02-16T22:13:27.706+0000] {standard_task_runner.py:85} INFO - Job 34: Subtask pyspark_consumer
[2025-02-16T22:13:27.956+0000] {task_command.py:416} INFO - Running <TaskInstance: kafka_spark_dag.pyspark_consumer manual__2025-02-16T22:11:30.822967+00:00 [running]> on host de5e2408bcb1
[2025-02-16T22:13:28.244+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='kafka_spark_dag' AIRFLOW_CTX_TASK_ID='pyspark_consumer' AIRFLOW_CTX_EXECUTION_DATE='2025-02-16T22:11:30.822967+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-02-16T22:11:30.822967+00:00'
[2025-02-16T22:13:28.356+0000] {docker.py:343} INFO - Starting docker container from image rappel-conso/spark:latest
[2025-02-16T22:13:30.599+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m22:13:30.57 [0m[38;5;2mINFO [0m ==>
[2025-02-16T22:13:30.633+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m22:13:30.62 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2025-02-16T22:13:30.665+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m22:13:30.64 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2025-02-16T22:13:30.702+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m22:13:30.69 [0m[38;5;2mINFO [0m ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.
[2025-02-16T22:13:30.749+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m22:13:30.73 [0m[38;5;2mINFO [0m ==>
[2025-02-16T22:13:30.901+0000] {docker.py:413} INFO - 
[2025-02-16T22:13:48.532+0000] {docker.py:413} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-02-16T22:13:48.873+0000] {docker.py:413} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2025-02-16T22:13:48.906+0000] {docker.py:413} INFO - org.postgresql#postgresql added as a dependency
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2025-02-16T22:13:48.910+0000] {docker.py:413} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-4d293cbc-19ce-447f-ad79-87887a941da7;1.0
	confs: [default]
[2025-02-16T22:13:51.013+0000] {docker.py:413} INFO - found org.postgresql#postgresql;42.5.4 in central
[2025-02-16T22:13:51.311+0000] {docker.py:413} INFO - found org.checkerframework#checker-qual;3.5.0 in central
[2025-02-16T22:13:54.282+0000] {docker.py:413} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central
[2025-02-16T22:13:55.241+0000] {docker.py:413} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
[2025-02-16T22:13:55.600+0000] {docker.py:413} INFO - found org.apache.kafka#kafka-clients;3.4.1 in central
[2025-02-16T22:13:55.934+0000] {docker.py:413} INFO - found org.lz4#lz4-java;1.8.0 in central
[2025-02-16T22:13:56.198+0000] {docker.py:413} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2025-02-16T22:13:57.442+0000] {docker.py:413} INFO - found org.slf4j#slf4j-api;2.0.7 in central
[2025-02-16T22:13:59.703+0000] {docker.py:413} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2025-02-16T22:14:00.278+0000] {docker.py:413} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2025-02-16T22:14:02.447+0000] {docker.py:413} INFO - found commons-logging#commons-logging;1.1.3 in central
[2025-02-16T22:14:02.703+0000] {docker.py:413} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-02-16T22:14:05.934+0000] {docker.py:413} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2025-02-16T22:14:06.093+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.5.4/postgresql-42.5.4.jar ...
[2025-02-16T22:14:06.644+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.postgresql#postgresql;42.5.4!postgresql.jar (591ms)
[2025-02-16T22:14:06.714+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar ...
[2025-02-16T22:14:06.937+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0!spark-sql-kafka-0-10_2.12.jar (297ms)
[2025-02-16T22:14:07.006+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar ...
[2025-02-16T22:14:07.176+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.checkerframework#checker-qual;3.5.0!checker-qual.jar (235ms)
[2025-02-16T22:14:07.255+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar ...
[2025-02-16T22:14:07.383+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0!spark-token-provider-kafka-0-10_2.12.jar (195ms)
[2025-02-16T22:14:07.483+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[2025-02-16T22:14:09.036+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (1610ms)
[2025-02-16T22:14:09.098+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2025-02-16T22:14:09.175+0000] {docker.py:413} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (134ms)
[2025-02-16T22:14:09.246+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2025-02-16T22:14:09.355+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (174ms)
[2025-02-16T22:14:09.438+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[2025-02-16T22:14:18.503+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (9129ms)
[2025-02-16T22:14:18.582+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2025-02-16T22:14:18.838+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (313ms)
[2025-02-16T22:14:18.898+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[2025-02-16T22:14:19.528+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (686ms)
[2025-02-16T22:14:19.593+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
[2025-02-16T22:14:19.664+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (130ms)
[2025-02-16T22:14:19.732+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[2025-02-16T22:14:25.138+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (5464ms)
[2025-02-16T22:14:25.210+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2025-02-16T22:14:25.287+0000] {docker.py:413} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (136ms)
[2025-02-16T22:14:25.291+0000] {docker.py:413} INFO - :: resolution report :: resolve 17121ms :: artifacts dl 19261ms
[2025-02-16T22:14:25.293+0000] {docker.py:413} INFO - :: modules in use:
[2025-02-16T22:14:25.294+0000] {docker.py:413} INFO - com.google.code.findbugs#jsr305;3.0.0 from central in [default]
[2025-02-16T22:14:25.297+0000] {docker.py:413} INFO - commons-logging#commons-logging;1.1.3 from central in [default]
[2025-02-16T22:14:25.299+0000] {docker.py:413} INFO - org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2025-02-16T22:14:25.300+0000] {docker.py:413} INFO - org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
	org.checkerframework#checker-qual;3.5.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.postgresql#postgresql;42.5.4 from central in [default]
[2025-02-16T22:14:25.309+0000] {docker.py:413} INFO - org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
[2025-02-16T22:14:25.310+0000] {docker.py:413} INFO - |      default     |   13  |   13  |   13  |   0   ||   13  |   13  |
[2025-02-16T22:14:25.310+0000] {docker.py:413} INFO - ---------------------------------------------------------------------
[2025-02-16T22:14:25.332+0000] {docker.py:413} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-4d293cbc-19ce-447f-ad79-87887a941da7
	confs: [default]
[2025-02-16T22:14:25.664+0000] {docker.py:413} INFO - 13 artifacts copied, 0 already retrieved (58001kB/334ms)
[2025-02-16T22:14:26.896+0000] {docker.py:413} INFO - 25/02/16 22:14:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-02-16T22:14:29.303+0000] {docker.py:413} INFO - python3: can't open file '/opt/bitnami/spark/scripts/spark_pgsql.py': [Errno 2] No such file or directory
[2025-02-16T22:14:29.374+0000] {docker.py:413} INFO - 25/02/16 22:14:29 INFO ShutdownHookManager: Shutdown hook called
[2025-02-16T22:14:29.379+0000] {docker.py:413} INFO - 25/02/16 22:14:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-7d597578-9c87-4107-a241-621eefe90570
[2025-02-16T22:14:30.432+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 486, in execute
    return self._run_image()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 360, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 421, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 2}
[2025-02-16T22:14:30.444+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=kafka_spark_dag, task_id=pyspark_consumer, execution_date=20250216T221130, start_date=20250216T221327, end_date=20250216T221430
[2025-02-16T22:14:30.519+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 34 for task pyspark_consumer (Docker container failed: {'StatusCode': 2}; 1761)
[2025-02-16T22:14:30.576+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-02-16T22:14:30.718+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
